{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from ultil_re import *\n",
    "from typing import Dict\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification, InputFeatures, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model1 sử dụng để predict có relation hay không\n",
    "model_path1= '/media/data3/users/longnd/ehr-relation-extraction/biobert_re/output/PubmedBert_Task1_'\n",
    "# model2 sử dụng để predict loại relation\n",
    "\n",
    "model_path2= '/media/data3/users/longnd/ehr-relation-extraction/biobert_re/output/PubmedBert_Task2_alpha(0.35)'\n",
    "tokenize_path= '/media/data3/users/longnd/ehr-relation-extraction/biobert_re/model/PubmedBertTokenizer'\n",
    "\n",
    "label1= ['No_Relation', 'Relation']\n",
    "label2= ['Association', 'Bind', 'Comparison', 'Conversion', 'Cotreatment', 'Drug_Interaction', 'Negative_Correlation', 'Positive_Correlation']\n",
    "label_map1: Dict[int, str] = {i: label for i, label in enumerate(label1)}\n",
    "label_map2: Dict[int, str] = {i: label for i, label in enumerate(label2)}\n",
    "num_label1= len(label1)\n",
    "num_label2= len(label2)\n",
    "\n",
    "config1 = AutoConfig.from_pretrained(model_path1, num_labels=num_label1, \n",
    "                                    id2label=label_map1, label2id={label: i for i, label in enumerate(label1)})\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenize_path)\n",
    "\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(model_path1, config=config1)\n",
    "\n",
    "config2 = AutoConfig.from_pretrained(model_path2, num_labels=num_label2, \n",
    "                                    id2label=label_map2, label2id={label: i for i, label in enumerate(label2)})\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(model_path2, config=config2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input của model đã được xử lý\n",
    "feature= open_pickle('/media/data3/users/longnd/ehr-relation-extraction/biobert_re/data/PubmedBert_Process/Test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids: List[int], attention_mask: List[int], token_type_ids: Optional[List[int]] = None, \n",
    "                 label_ids: Optional[int] = None   ):\n",
    "        self.input_ids= input_ids # list of token ids\n",
    "        self.attention_masks= attention_mask # list of attention mask\n",
    "        self.token_type_ids= token_type_ids # list of token type ids\n",
    "        self.labels= label_ids # list of label ids\n",
    "class REDatasetForPredict(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for relation extraction\n",
    "    \"\"\"\n",
    "    features: List[InputFeatures]\n",
    "    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n",
    "    def __init__(self, features: List[InputFeatures]):\n",
    "        self.features = features\n",
    "        self.input_ids = [torch.tensor(example.input_ids).long() for example in self.features]\n",
    "        self.attention_masks = [torch.tensor(example.attention_mask).float() for example in self.features]\n",
    "        self.token_type_ids = [torch.tensor(example.token_type_ids).long() for example in self.features]\n",
    "        # self.labels = [torch.tensor(example.labels).long() for example in self.features]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[index],\n",
    "            'attention_masks': self.attention_masks[index],\n",
    "            'token_type_ids': self.token_type_ids[index]\n",
    "            }\n",
    "\n",
    "def convert_raw_text(text:str, list_en1: List[str], list_en2: List[str]):\n",
    "    \"\"\"\"\"\"\n",
    "    list_en= list_en1+ list_en2\n",
    "    list_en= sorted(list_en, key= lambda x: x[0])   \n",
    "    new_text= ''\n",
    "    begin= 0\n",
    "    for i in list_en:\n",
    "        # (start, end, type, ent_text)\n",
    "        assert text[i[0]:i[1]] == i[3], 'start and end of entity is not correct'\n",
    "        new_text+= text[begin:i[0]]\n",
    "        new_text+= '@'+i[2]+'$'+' '+ i[3]+' '+'@/'+i[2]+'$'\n",
    "        begin= i[1]\n",
    "    new_text+= text[begin:]\n",
    "    return new_text\n",
    "\n",
    "def convert_text_to_input_bert(text, entity_list, tokenizer, max_seq_length=512):\n",
    "    relation_pair= [['GeneOrGeneProduct', 'GeneOrGeneProduct'], ['GeneOrGeneProduct', 'DiseaseOrPhenotypicFeature'],\n",
    "                    ['GeneOrGeneProduct', 'ChemicalEntity'], ['DiseaseOrPhenotypicFeature', 'SequenceVariant'],\n",
    "                    ['ChemicalEntity', 'DiseaseOrPhenotypicFeature'], ['ChemicalEntity', 'ChemicalEntity'],\n",
    "                    ['ChemicalEntity', 'SequenceVariant'], ['SequenceVariant', 'SequenceVariant'],\n",
    "                    ['GeneOrGeneProduct', 'SequenceVariant']]\n",
    "    feature= []\n",
    "    # entity_list: {code: (N, start, end, type, ent_text)}\n",
    "    code_list = []\n",
    "    keys= list(entity_list.keys())\n",
    "    for i in range(len(keys)):\n",
    "            entity1= entity_list[i]\n",
    "            ent1= entity1[0][2]\n",
    "            for j in range(i, len(keys)):\n",
    "                entity2= entity_list[j]\n",
    "                ent2= entity2[0][2]\n",
    "                code_list.append(keys[i], keys[j])\n",
    "                if ((ent1, ent2) not in relation_pair) and ((ent1, ent2) not in relation_pair):\n",
    "                    continue\n",
    "                sequence= convert_raw_text(text, entity1, entity2)\n",
    "                encoded_input = tokenizer.encode_plus(\n",
    "                                sequence,\n",
    "                                add_special_tokens=True,\n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length= max_seq_length,\n",
    "                                return_attention_mask=True,\n",
    "                                return_token_type_ids=True)\n",
    "                input_ids = encoded_input[\"input_ids\"]\n",
    "                token_type_ids = encoded_input[\"token_type_ids\"]\n",
    "                attention_mask = encoded_input[\"attention_mask\"]\n",
    "                feature.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids= token_type_ids, label_ids= 0)) \n",
    "    return feature, code_list\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset= REDatasetForPredict(features= feature)\n",
    "data = DataLoader(test_dataset, batch_size= 1, shuffle= False, num_workers= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model1, model2, data_loader, relation_pair, device='cuda:0'):\n",
    "    '''Model 1: no relation and relation'''\n",
    "    model1 = model1.to(device)\n",
    "    model2 = model2.to(device)\n",
    "    # batch_preds = np.empty((0,), dtype=np.int64)\n",
    "    batch_preds = []\n",
    "    pos_pair= {'G-G':['Association','Bind', 'Negative_Correlation','Positive_Correlation', 'No_Relation'],\n",
    "            'G-D':['Association', 'Negative_Correlation','Positive_Correlation', 'No_Relation'],\n",
    "            'C-D':['Association', 'Negative_Correlation','Positive_Correlation', 'No_Relation'],\n",
    "            'C-C':['Association', 'Bind', 'Comparison', 'Conversion', 'Cotreatment', 'Drug_Interaction', \n",
    "                        'Negative_Correlation', 'Positive_Correlation', 'No_Relation'],\n",
    "            'G-C':['Association','Bind', 'Negative_Correlation','Positive_Correlation', 'No_Relation'],\n",
    "            'D-V':['Association', 'Negative_Correlation','Positive_Correlation', 'No_Relation'],\n",
    "            'G-V':['No_Relation'],\n",
    "            'V-V':['Association', 'No_Relation'],\n",
    "            'C-V':['Association', 'Negative_Correlation','Positive_Correlation', 'No_Relation']}\n",
    "    label= ['Association', 'Bind', 'Comparison', 'Conversion', 'Cotreatment', 'Drug_Interaction', 'Negative_Correlation', \n",
    "                'Positive_Correlation', 'No_Relation']\n",
    "    label2id = {l:i for i, l in enumerate(label)}\n",
    "    pos_pair2id = {}\n",
    "    for k, v in pos_pair.items():\n",
    "        pos_pair2id[k] = [label2id[i] for i in v if label2id[i]!= 8]\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model1(batch['input_ids'].to(device), batch['attention_masks'].to(device), batch['token_type_ids'].to(device))\n",
    "            preds = np.argmax(outputs[0].detach().cpu().numpy(), axis=1)\n",
    "            if preds[0] == 0:\n",
    "                preds= 8\n",
    "            else:\n",
    "                outputs = model2(batch['input_ids'].to(device), batch['attention_masks'].to(device), batch['token_type_ids'].to(device))\n",
    "                logits= outputs[0][0].detach().cpu().numpy()#[pos_pair2id[relation_pair[idx]]]\n",
    "                preds = np.argmax(logits, axis=0)\n",
    "            batch_preds.append(preds)         \n",
    "    return batch_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8433/8433 [02:24<00:00, 58.54it/s]\n"
     ]
    }
   ],
   "source": [
    "labels= get_labels()\n",
    "labels_map= {i: label for i, label in enumerate(labels)}\n",
    "pred= predict(model1, model2, data, relation_pair)\n",
    "prediction= [labels_map[i] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_pair= get_pair_relation(pubtator_file= '/media/data3/users/longnd/ehr-relation-extraction/biobert_re/data/BioRED/Test.PubTator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test= []\n",
    "for i in feature:\n",
    "    label_test.append(i.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == len(relation_pair) == len(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C-C', 'C-D', 'C-V', 'D-V', 'G-C', 'G-D', 'G-G', 'G-V', 'V-V'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(relation_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Association', 'Bind', 'Comparison', 'Conversion', 'Cotreatment', 'Drug_Interaction', 'Negative_Correlation', 'Positive_Correlation', 'No_Relation']\n"
     ]
    }
   ],
   "source": [
    "print(get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G-G    3470\n",
       "G-D    1263\n",
       "C-D     694\n",
       "C-C     670\n",
       "G-C     664\n",
       "D-V     557\n",
       "G-V     544\n",
       "V-V     404\n",
       "C-V     167\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(relation_pair).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp, p_true, p_pred\n",
    "a= {i: [0, 0, 0] for i in set(relation_pair)}\n",
    "for ent in a.keys():\n",
    "    for i in range(len(label_test)):\n",
    "        if label_test[i]== pred[i] and relation_pair[i]== ent and label_test[i]!= 8:\n",
    "            a[ent][0]+= 1\n",
    "        if label_test[i]!= 8 and relation_pair[i]== ent:\n",
    "            a[ent][1]+= 1\n",
    "        if pred[i]!= 8 and relation_pair[i]== ent:\n",
    "            a[ent][2]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-C\n",
      "     precision:  0.5967741935483871\n",
      "     recall:  0.5522388059701493\n",
      "     f1:  0.5736434108527133\n",
      "G-G\n",
      "     precision:  0.34935897435897434\n",
      "     recall:  0.29222520107238603\n",
      "     f1:  0.3182481751824817\n",
      "G-C\n",
      "     precision:  0.45098039215686275\n",
      "     recall:  0.5054945054945055\n",
      "     f1:  0.4766839378238342\n",
      "G-V\n",
      "     precision:  0.42592592592592593\n",
      "     recall:  0.3709677419354839\n",
      "     f1:  0.39655172413793105\n",
      "V-V\n",
      "     precision:  0.2857142857142857\n",
      "     recall:  0.23529411764705882\n",
      "     f1:  0.2580645161290323\n",
      "C-V\n",
      "     precision:  0.30434782608695654\n",
      "     recall:  0.19444444444444445\n",
      "     f1:  0.23728813559322037\n",
      "C-D\n",
      "     precision:  0.5703125\n",
      "     recall:  0.474025974025974\n",
      "     f1:  0.5177304964539007\n",
      "G-D\n",
      "     precision:  0.43037974683544306\n",
      "     recall:  0.38636363636363635\n",
      "     f1:  0.40718562874251496\n",
      "D-V\n",
      "     precision:  0.4166666666666667\n",
      "     recall:  0.3488372093023256\n",
      "     f1:  0.379746835443038\n"
     ]
    }
   ],
   "source": [
    "for i in a.keys():\n",
    "    tp, p_true, p_pred= a[i]\n",
    "    \n",
    "    pre= tp/p_pred\n",
    "    rec= tp/p_true\n",
    "    print(i)\n",
    "    print('     precision: ', pre)\n",
    "    print('     recall: ', rec)\n",
    "    print('     f1: ', 2*pre*rec/(pre+rec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
