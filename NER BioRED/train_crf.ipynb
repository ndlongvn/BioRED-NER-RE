{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 18:07:19.838754: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-20 18:07:21.248743: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-20 18:07:21.248794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-20 18:07:21.248887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-20 18:07:21.248993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-08-20 18:07:21.249011: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-20 18:07:21.250414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-20 18:07:21.250447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-20 18:07:21.270524: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-20 18:07:21.270765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-20 18:07:21.272190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-20 18:07:21.272942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-20 18:07:21.275777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-20 18:07:21.275857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-20 18:07:21.276008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-20 18:07:21.276096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    "    IntervalStrategy,\n",
    "    BertPreTrainedModel,\n",
    "    AutoModel,\n",
    "    BertModel,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "from ultil_ner import NerDataset, get_labels\n",
    "class BertCRFModel(nn.Module):\n",
    "    def __init__(self, config, model_name_or_path):\n",
    "        super(BertCRFModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name_or_path, config=config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss = self.crf(emissions = logits, tags=labels, mask=attention_mask.byte())\n",
    "            outputs =(-1*loss,)+outputs\n",
    "            return outputs \n",
    "        else:\n",
    "            return self.crf.decode(logits, attention_mask.byte())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label= {i: label for i, label in enumerate(get_labels())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/data3/users/longnd/ehr-relation-extraction/biobert_ner/model/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 0-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:29<00:00,  1.59it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:46<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "{'precision': 0.8493227990970654, 'recall': 0.8519671667138409, 'f1': 0.850642927794263, 'f1_macro': 0.850642927794263, 'recall_macro': 0.8519671667138409, 'precision_macro': 0.8493227990970654}\n",
      "-----------------Epoch: 1-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:24<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:46<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "{'precision': 0.862675089261192, 'recall': 0.8890461364279649, 'f1': 0.8756621131865068, 'f1_macro': 0.8756621131865068, 'recall_macro': 0.8890461364279649, 'precision_macro': 0.862675089261192}\n",
      "-----------------Epoch: 2-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 3-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 4-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:46<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 5-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:24<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:46<00:00, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 6-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:50<00:00,  1.48it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 7-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "{'precision': 0.8712351478308925, 'recall': 0.8924426832720068, 'f1': 0.8817114093959733, 'f1_macro': 0.8817114093959733, 'recall_macro': 0.8924426832720068, 'precision_macro': 0.8712351478308925}\n",
      "-----------------Epoch: 8-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:46<00:00, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 9-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 10-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:24<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 11-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:22<00:00,  1.64it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 12-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 13-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 14-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 15-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:23<00:00,  1.63it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:46<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 16-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 430/430 [04:50<00:00,  1.48it/s]\n",
      "Evaluating: 100%|██████████| 1106/1106 [00:45<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Epoch: 17-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 8/430 [00:05<04:54,  1.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 154\u001b[0m\n\u001b[1;32m    152\u001b[0m scheduler \u001b[39m=\u001b[39m CosineAnnealingLR(optimizer, T_max\u001b[39m=\u001b[39mnum_epochs)\n\u001b[1;32m    153\u001b[0m \u001b[39m# evaluate(model, eval_dataloader, device='cuda')\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m train(model, num_epochs, train_dataloader, eval_dataloader, optimizer, scheduler, device)\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epoch, train_dataloader, val_dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m loss \u001b[39m=\u001b[39m model(input_ids, attention_mask, token_type_ids, labels\u001b[39m=\u001b[39mlabels)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "output_dir= '/media/data3/users/longnd/ehr-relation-extraction/biobert_ner/output/pubmedbert-crf'\n",
    "device= 'cuda'\n",
    "# Training function\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def idtoLabel(pr, labels=id2label):\n",
    "    res= []\n",
    "    for i in pr:\n",
    "        res.append(labels[i])\n",
    "    return res\n",
    "def train(model, epoch, train_dataloader, val_dataloader, optimizer, scheduler,  device='cuda'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    f1 = 0\n",
    "    for i in range(epoch):\n",
    "        print('-----------------Epoch: '+ str(i)+ '-----------------')\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_ids, attention_mask, token_type_ids, labels=labels)[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        res= evaluate(model, val_dataloader, device)\n",
    "        scheduler.step()\n",
    "        if res['f1_macro']> f1:\n",
    "            print('Epoch: '+ str(i))\n",
    "            print(res)\n",
    "            f1= res['f1_macro']\n",
    "            torch.save(model.state_dict(), output_dir+'/epoch_'+ str(i)+ '_f1_'+ str(f1)+ '.pt')\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, eval_dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels']\n",
    "\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            predicted_labels = output[0]\n",
    "\n",
    "            predictions.append(idtoLabel(predicted_labels))\n",
    "            true_labels.append(idtoLabel(labels.numpy().tolist()[0][0: len(predicted_labels)]))\n",
    "            # if i<5: \n",
    "            #     print(predictions)\n",
    "            #     print(true_labels)\n",
    "\n",
    "    # Flatten the predictions and true labels lists\n",
    "    # predictions = [p for sublist in predictions for p in sublist]\n",
    "    # true_labels = [l for sublist in true_labels for l in sublist]\n",
    "\n",
    "    # Calculate metrics\n",
    "    return {\"precision\": precision_score(true_labels, predictions),\n",
    "            \"recall\": recall_score(true_labels, predictions),\n",
    "            \"f1\": f1_score(true_labels, predictions),\n",
    "            \"f1_macro\": f1_score(true_labels, predictions, average=\"macro\"),\n",
    "            \"recall_macro\": recall_score(true_labels, predictions, average=\"macro\"),\n",
    "            \"precision_macro\": precision_score(true_labels, predictions, average=\"macro\")\n",
    "            }\n",
    "                \n",
    "\n",
    "\n",
    "# Specify device (e.g., 'cuda' for GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "labels = get_labels()\n",
    "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "config = AutoConfig.from_pretrained(\n",
    "        '/media/data3/users/longnd/ehr-relation-extraction/biobert_ner/model/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        '/media/data3/users/longnd/ehr-relation-extraction/biobert_ner/model/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "        use_fast=False,\n",
    ")\n",
    "\n",
    "model= BertCRFModel(model_name_or_path='/media/data3/users/longnd/ehr-relation-extraction/biobert_ner/model/BiomedNLP-PubMedBERT-base-uncased-abstract', config=config)\n",
    "model.to(device)\n",
    "# Get datasets\n",
    "train_dataset =NerDataset(\n",
    "            data_dir='/media/data3/users/longnd/ehr-relation-extraction/biobert_ner/data/Preprocess_PubmedBert_6class',\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=512,\n",
    "            overwrite_cache=False,\n",
    "            mode= 'Train'\n",
    "        )\n",
    "\n",
    "eval_dataset =  NerDataset(\n",
    "            data_dir='/media/data3/users/longnd/ehr-relation-extraction/biobert_ner/data/Preprocess_PubmedBert_6class',\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=512,\n",
    "            overwrite_cache=False,\n",
    "            mode='Dev',\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [feature.input_ids for feature in batch]\n",
    "    attention_mask = [feature.attention_mask for feature in batch]\n",
    "    token_type_ids = [feature.token_type_ids for feature in batch]\n",
    "    label_ids = [feature.label_ids for feature in batch]\n",
    "\n",
    "    # Convert the label_ids to a tensor\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    token_type_ids = torch.tensor(token_type_ids)\n",
    "    label_ids = torch.tensor(label_ids)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids, 'labels': label_ids}\n",
    "\n",
    "    # return input_ids, attention_mask, token_type_ids, label_ids\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Specify training parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 7e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "# evaluate(model, eval_dataloader, device='cuda')\n",
    "train(model, num_epochs, train_dataloader, eval_dataloader, optimizer, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
